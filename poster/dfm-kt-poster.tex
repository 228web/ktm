\documentclass[25pt, a0paper,
margin=0mm, innermargin=15mm, blockverticalspace=5mm, colspace=15mm, subcolspace=1mm]{tikzposter}
\usepackage{trace}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{cmbright}
\usepackage[backend=biber]{biblatex}
\addbibresource{dfm-biblio.bib}
\usepackage{fontspec}
\usepackage{bm}
\newfontfeature{Microtype}{protrusion=default;expansion=default;}
\tikzposterlatexaffectionproofoff

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\probit{probit}
\def\R{\mathbf{R}}
\def\N{\mathcal{N}}
\def\ReLU{\textnormal{ReLU}}
\def\pij{p_{ij}}
\def\logitp{\logit{} p_{ij}}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    citecolor={blue!80!black},
}
\newcommand{\email}[1]{\href{mailto:#1}{\textsf{#1}}}
\title{\parbox{\linewidth}{\begin{center}\textbf{Deep Factorization Machines for Knowledge Tracing}\end{center}}}
\author{\centering
  \begin{tabular}{cc}
    \textbf{Jill-Jênn Vie} \\
    RIKEN Center for Advanced Intelligence Project (AIP)\\
Tokyo, Japan\\
\texttt{vie@jill-jenn.net}
  \end{tabular}
    }

\usetheme{Simple}

\usecolorpalette{BrownBlueOrange}
\makeatletter
\renewcommand\TP@maketitle{
   \begin{minipage}{0.98\linewidth}
        \centering
        \color{titlefgcolor}
        {\bfseries \Huge \sc \@title \par}
        \vspace*{1em}
        {\Large
          \@author \par}
    \end{minipage}
}
\makeatother

\usepackage{theorem}
\theoremstyle{myplain}
\newtheorem{transformation}{Transformation}
\newcommand{\concprio}{\rangle\kern-0.15em\rangle}

\usepackage{tikz-qtree}

\usetikzlibrary{arrows,arrows.meta,automata,shapes,intersections,mindmap,trees,shadows,fit,positioning,calc,matrix,decorations,chains,external,3d}
\makeatletter
\tikzoption{canvas is xy plane at z}[]{%
\def\tikz@plane@origin{\pgfpointxyz{0}{0}{#1}}%
\def\tikz@plane@x{\pgfpointxyz{1}{0}{#1}}%
\def\tikz@plane@y{\pgfpointxyz{0}{1}{#1}}%
\tikz@canvas@is@plane
}
\makeatother
\usepackage{listings}
\usepackage{enumitem}
\usepackage{booktabs}
\newcommand{\Gproc}{\textbf{proc }}
\newcommand{\GendProc}{\textbf{ endProc}}
\newcommand{\Gif}{\textbf{if }}
\newcommand{\Gthen}{\textbf{ then }}
\newcommand{\Gelse}{\textbf{ else }}
\newcommand{\Gwhile}{\textbf{while }}
\newcommand{\Gdo}{\textbf{ do }}
\newcommand{\mprefer}{\,\rangle\,}
\newcommand{\GendIf}{\textbf{ endIf }}
\newcommand{\GendWhile}{\textbf{ endWhile }}
\newcommand{\affp}{\textit{aff\/}_p}
\newcommand{\set}[1]{\{#1\}}

\newcommand{\Gleft}[1]{G_{#1}^{\textit{left}}}
\newcommand{\Gright}[1]{G_{#1}^{\textit{right}}}
\newcommand{\Gmiddle}[1]{G_{#1}}

\newcommand{\prefer}{\rangle}

\newcommand{\alert}[1]{{\color{red!70!black} #1}}

\newcommand{\bs}{\textbackslash}   % backslash
\newcommand{\cmd}[1]{{\bf \color{red}#1}}   % highlights command

\graphicspath{{./}{../images/}}

\usenotestyle{Sticky}

\usepackage{mdframed}
\newmdenv[topline=false,bottomline=false,rightline=false,skipabove=0pt,skipbelow=0pt,innertopmargin=0pt,innerbottommargin=0pt]{sidebox}
\def\HyperFirstAtBeginDocument#1{#1}  % lolwut https://tex.stackexchange.com/a/309895/7144

\begin{document}
\maketitle[width=\linewidth,titletotopverticalspace=0pt]

% Here it starts

\definecolor{pixblue}{RGB}{53,81,250}
\colorlet{innerblocktitlebgcolor}{pixblue}

\begin{columns} % Blocks will be placed into columns
    
    \column{.48}
    \block[roundedcorners=40]{Problem: Knowledge Tracing for Language Learning}{
        We want to \alert{predict the correctness} of students over words.\\
        Each student can attempt to write a certain word multiple times, and learns in-between.\\
        \textbf{Fit:} Ordered triplets $(i, j, o) \in I \times J \times \{0, 1\}$\\
        $\Rightarrow$ Student $i$ attempted word $j$ and wrote it correctly/incorrectly.\\
        \textbf{Predict:} $(i, j, ?)$ for new triplets.
    }
    \block[roundedcorners=40]{Existing families of models}{
        \begin{itemize}
            \item \textbf{Prediction of sequences}: Bayesian Knowledge Tracing (BKT $:=$ HMM)\\ Deep Knowledge Tracing (DKT $:=$ LSTM) \autocite{piech2015deep}
            \item \textbf{Factor Analysis}: Item Response Theory (IRT), Performance Factor Analysis (PFA)

            \[ \textnormal{BKT} < \textnormal{PFA} \simeq^{\textnormal{\autocite{xiong2016going}}} \textnormal{DKT} \leq^{\textnormal{\cite{wilson2016back}}} \textnormal{IRT } \alert{\leq^{\textnormal{[this poster]}} \textnormal{FM}} \]

        \end{itemize}

        \vspace{1cm}
        \innerblock[]{Logistic Regression}{
            All students $i \in I$, questions $j \in J$ and metadata are encoded into sparse features $x$\\
            Each feature $k$ has a bias \alert{$w_k$}
            \[ \logit p(x) = \logit \Pr(\textsf{event } x \textnormal{ has positive outcome}) = \mu + \alert{w^T} x \]

            \hfill $\Rightarrow$ \textnormal{ \textbf{really simple, ignores pairwise interactions}}
        }
        Particular cases for user $i$ against token $j$:\\
        Item response theory (IRT): $\logit{} p_{ij} = \alert{\theta_i} - \alert{d_j}$\\
        Performance Factor Analysis (PFA): $\logit{} p_{ij} = \sum_{k \in \textnormal{KC}(j)} \alert{\beta_k} + \alert{\gamma_k} W_{ik} + \alert{\delta_k} F_{ik}$\\
        \vspace{1cm}
        \innerblock[]{Factorization Machines}{
            All students $i \in I$ and questions $j \in J$ and past performance are encoded into $x$\\
            All entities have a bias \alert{$w_k$} and features \alert{$\bm{v_k}$} to model pairwise relationships between them

            \[ \psi\left(p(x)\right) = \mu + \underbrace{\sum_{k = 1}^N \alert{w_k} x_k}_{\textnormal{logistic regression}} + \underbrace{\sum_{1 \leq k < l \leq N} x_k x_l \langle \alert{\bm{v_k}}, \alert{\bm{v_l}} \rangle}_{\textnormal{pairwise interactions}} \]
            \hfill $\Rightarrow$ \textnormal{ \textbf{converting sparse features to dense embeddings}}
        }
        Particular cases:\\
        Multidimensional Item Response Theory (MIRT): $\logit{} p_{ij}= \langle \alert{\bm{\theta_i}}, \alert{\bm{d_j}} \rangle + \alert{\delta_j}$\\
        BFM (Bayesian Factorization Machines \cite{freudenthaler2011bayesian}) for classification
        \begin{itemize}
            \item If $\psi = \probit$, $w_k, v_{kf} \sim \N(\mu, 1/\lambda)$, $\mu \sim \N(0, 1)$, $\lambda \sim \Gamma(1, 1)$,\\
            a \textbf{Gibbs sampler} allows to train efficiently KTM \cite{rendle2012factorization}
        \end{itemize}
    }

    \column{.52}
    \block{Our proposal \autocite{guo2017deepfm}}{
        \innerblock[]{Deep Factorization Machines}{
            All students $i \in I$ and questions $j \in J$ and past performance are encoded into $x$\\
            All entities have a bias \alert{$w_k$} and features \alert{$\bm{v_k}$} to model pairwise relationships between them

            \begin{gather*}
            \psi(p(x)) = y_{FM}(x) + y_{DNN}(x)\\
            y_{FM}(x) = \mu + \sum_{k = 1}^N \alert{w_k} x_k + \sum_{1 \leq k < l \leq N} x_k x_l \langle \alert{\bm{v_k}}, \alert{\bm{v_l}} \rangle\\
            y_{DNN}(x) = \ReLU(\alert{W^{(L)}} a^{(L)} + \alert{b^{(L)}})\\
            a^{(\ell + 1)}(x) = \ReLU(\alert{W^{(\ell)}} a^{(\ell)} + \alert{b^{(\ell)}})\\
            a^{0}(x) = (v_{user}, v_{token}, \ldots)
            \end{gather*}
        }
    }

    \block{Encoding of entities}{
        Unsupervised problem becomes a supervised problem:

        \vspace{5mm}
        \begin{center}
        \input{tables/fm-poster}
        \end{center}
        \vspace{1cm}

        \begin{minipage}{0.6\linewidth}
            Encoding users + items == IRT\\
            Encoding users + skills + attempts == AFM\\
            Encoding users + skills + wins + fails == PFA\bigskip\\

            How much do we earn with each of these nested models?
        \end{minipage}
        \begin{minipage}{0.4\linewidth}
            \includegraphics[width=\linewidth]{figures/aip.png}
        \end{minipage}
    }

\end{columns}
\begin{columns}
\column{.55}
\block{Results on large-scale Duolingo dataset}{
    \centering
    \input{tables/results-fr_en}
    \input{tables/results-es_en}
    \input{tables/results-en_es}
}


\column{.45}
\block{Better models found}{
    \begin{itemize}
    \item Pairwise interactions are useful
    \item Deep does not help much
    \item Time and days harm
    \end{itemize}
}
\block{Optimizing Human Learning}{
    We are organizing a workshop in Montréal on \textbf{June 12}:\\
    CFP open on \url{humanlearn.io}
}
\block{References}{
    \printbibliography[heading=none]
}
\end{columns}

\end{document}

\endinput
